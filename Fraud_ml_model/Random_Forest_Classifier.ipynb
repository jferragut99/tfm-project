{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "from pathlib import Path\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "from email.mime.text import MIMEText\n",
    "import smtplib\n",
    "home = str(Path.home())\n",
    "t1 = time.time()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, recall_score, make_scorer, precision_recall_curve\n",
    "from ydata_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake connector dbt_jferragut\n",
    "engine = utils.create_snowflake_engine(TAG = ' ')\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_sql = \"\"\"\n",
    "SELECT *\n",
    "FROM  test_hbg.dbt_jferragut.FEATURE_ENCODED\"\"\"\n",
    "\n",
    "base_data = pd.read_sql_query(base_data_sql,engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.utils import resample\n",
    "X = base_data.drop(['bookingreference', 'booking_timestamp', 'is_fraud'], axis = 1)\n",
    "y = base_data['is_fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y) # constant ratio in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_grid_search(estimator, param_grid, X, y, set_sizes_grid,file_name=\"grid_search_results.txt\"):\n",
    "    \"\"\"\n",
    "    Manual implementation of GridSearchCV functionality.\n",
    "    \n",
    "    Parameters:\n",
    "    - estimator: The machine learning model (e.g., an instance of sklearn's classifiers).\n",
    "    - param_grid: Dictionary where keys are parameter names and values are lists of parameter settings to try.\n",
    "    - X: Features dataset.\n",
    "    - y: Target dataset.\n",
    "    - scoring: Metric function to evaluate the model (default: accuracy_score).\n",
    "    - file_name: Name of the output .txt file to save results.\n",
    "    \"\"\"\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Get all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(itertools.product(*param_values))\n",
    "    # Get the current timestamp\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "    # Iterate over all parameter combinations\n",
    "    for combination in param_combinations:\n",
    "        # Create a dictionary of parameters\n",
    "        params = dict(zip(param_names, combination))\n",
    "        \n",
    "        # Set the parameters to the estimator\n",
    "        estimator.set_params(**params)\n",
    "        \n",
    "        # Train the model\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions and calculate the score\n",
    "        y_pred = estimator.predict(X_test)\n",
    "        score = classification_report(y_test, y_pred)\n",
    "        \n",
    "       \n",
    "         # Open the file to write results\n",
    "        with open(file_name, \"a\") as file:\n",
    "            # Write the results to the file\n",
    "            file.write(f\"{timestamp}\\t{set_sizes_grid}\\t{params}\\n{score}\\n\")\n",
    "        \n",
    "        # Optionally, print the results to the console\n",
    "        # print(f\"Timestamp: {timestamp}, Parameters: {params}, Score: {score}\")\n",
    "\n",
    "    print(f\"Results have been saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sampled, _, y_train_sampled, _ = train_test_split(X_train,y_train, test_size=0.9, random_state=42, stratify=y_train) # constant ratio in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid ={\n",
    "    'n_estimators': [200, 150, 100], # number of trees\n",
    "    'max_depth': [10], # max depth for each tree (None-> with no limit)\n",
    "    'min_samples_split': [2], # minimum samples required to split a node\n",
    "    'min_samples_leaf': [12, 15, 20, 25], # minimum samples required in a leaf (terminal node)\n",
    "    # ,'min_weight_fraction_leaf' :[0,0.001]\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'class_weight': [{0: 689, 1: 3360347}, {0: 1, 1: 1000}, {0: 1, 1: 10000}]\n",
    "}\n",
    "set_sizes_grid = [0.7] # 600k, 900k, 1.2M , 0.7, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(random_state=42)\n",
    "# manual_grid_search(rf,param_grid, X_train_sampled, y_train_sapled, file_name=\"grid_search_results.txt\" ,set_sizes_grid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set_sizes_grid:\n",
    "    X_train_sampled, _, y_train_sampled, _ = train_test_split(X_train,y_train, test_size=i, random_state=42, stratify=y_train) # constant ratio in train and test\n",
    "    manual_grid_search(rf,param_grid, X_train_sampled, y_train_sampled, file_name=\"grid_search_results.txt\",set_sizes_grid=i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model with best params\n",
    "final_model = RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=10,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=12,\n",
    "        criterion='gini', \n",
    "        class_weight={0:1,1:1000}, \n",
    "        random_state=42)\n",
    "final_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_train = final_model.predict_proba(X_train)[:,1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_scores_train)\n",
    "\n",
    "for p, r, t in zip(precision, recall, thresholds):\n",
    "    if r > 0.8 and p >= 0.01:\n",
    "        print(f\"Threshold: {t: .2f}, precision: {p: .2f}, recall: {r: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define threshold based on what we see in previous cell results\n",
    "threshold = 0.4\n",
    "y_pred_train = (y_scores_train > threshold).astype(int)\n",
    "print(len(y_pred_train))\n",
    "print(classification_report(y_train,y_pred_train, target_names=['No Fraud','Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in train\n",
    "y_pred_train = final_model.predict(X_train)\n",
    "print(classification_report(y_train,y_pred_train, target_names=['No Fraud','Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in test\n",
    "threshold = 0.45\n",
    "y_scores = final_model.predict_proba(X_test)[:,1]\n",
    "y_pred = (y_scores > threshold).astype(int)\n",
    "print(len(y_pred))\n",
    "print(classification_report(y_test,y_pred, target_names=['No Fraud','Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in test\n",
    "y_pred = final_model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred, target_names=['No Fraud','Fraud']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
